name: Deploy DAGs and Streaming Pipeline

on:
  push:
    branches: [ main ]
    paths:
      - 'dags/**'
      - 'scripts/streaming/**'
      - 'scripts/setup/**'
      - 'cloudbuild.yaml'

jobs:
  deploy-dags:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install google-cloud-storage python-dotenv
    
    - name: Create .env file
      run: |
        echo "GCP_PROJECT_ID=${{ secrets.GCP_PROJECT_ID }}" > .env
        echo "RAW_DATA_BUCKET=${{ secrets.RAW_DATA_BUCKET }}" >> .env
        echo "PROCESSED_DATA_BUCKET=${{ secrets.PROCESSED_DATA_BUCKET }}" >> .env
        echo "ARCHIVE_DATA_BUCKET=${{ secrets.ARCHIVE_DATA_BUCKET }}" >> .env
        echo "IMDB_DATASETS=${{ secrets.IMDB_DATASETS }}" >> .env
        echo "BIGQUERY_DATASET=${{ secrets.BIGQUERY_DATASET }}" >> .env
        echo "GCP_REGION=${{ secrets.GCP_REGION }}" >> .env
    
    - name: Setup GCP Credentials
      run: |
        echo '${{ secrets.GOOGLE_APPLICATION_CREDENTIALS }}' > /tmp/gcp-credentials.json
        echo "GOOGLE_APPLICATION_CREDENTIALS=/tmp/gcp-credentials.json" >> $GITHUB_ENV
    
    - name: Deploy DAGs to Composer
      env:
        GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
        GCP_COMPOSER_BUCKET: ${{ secrets.GCP_COMPOSER_BUCKET }}
      run: |
        python scripts/deploy_dags_to_composer.py

  deploy-dataflow:
    runs-on: ubuntu-latest
    # This job doesn't need to depend on deploy-dags
    steps:
    - uses: actions/checkout@v2
    
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.9'
    
    - name: Setup GCP Credentials
      run: |
        echo '${{ secrets.GOOGLE_APPLICATION_CREDENTIALS }}' > /tmp/gcp-credentials.json
        echo "GOOGLE_APPLICATION_CREDENTIALS=/tmp/gcp-credentials.json" >> $GITHUB_ENV
    
    - name: Setup gcloud CLI
      uses: google-github-actions/setup-gcloud@v0
      with:
        project_id: ${{ secrets.GCP_PROJECT_ID }}
        service_account_key: ${{ secrets.GOOGLE_APPLICATION_CREDENTIALS }}
        export_default_credentials: true
    
    - name: Authenticate Docker to GCR
      run: gcloud auth configure-docker
    
    - name: Setup Environment
      run: |
        # Install dependencies
        pip install google-cloud-bigquery google-cloud-pubsub
        
        # Create BigQuery events table if it doesn't exist
        python scripts/setup/create_events_tables.py
        
        # Create Pub/Sub topics and subscriptions if they don't exist
        python scripts/setup/create_pubsub_topics.py
    
    - name: Submit Cloud Build
      env:
        PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
        REGION: ${{ secrets.GCP_REGION }}
        PROCESSED_DATA_BUCKET: ${{ secrets.PROCESSED_DATA_BUCKET }}
      run: |
        # Use the variable name that matches what's in cloudbuild.yaml
        gcloud builds submit \
          --config=cloudbuild.yaml \
          --substitutions=_PROCESSED_DATA_BUCKET=${PROCESSED_DATA_BUCKET} \
          .
    
    - name: Test Event Simulator
      if: false  # Disabled by default, set to true to enable testing
      run: |
        pip install google-cloud-pubsub
        python scripts/streaming/event_simulator.py \
          --project_id=${{ secrets.GCP_PROJECT_ID }} \
          --topic_name=ott-raw-events \
          --user_count=10 \
          --events_per_user=5 \
          --rate_limit=1