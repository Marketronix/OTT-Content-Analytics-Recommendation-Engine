# Updated cloudbuild.yaml with proper template metadata
steps:
  # Build the Docker image
  - name: 'gcr.io/cloud-builders/docker'
    args: ['build', '-t', 'gcr.io/${PROJECT_ID}/ott-event-pipeline:latest', './scripts/streaming']
  
  # Push the Docker image
  - name: 'gcr.io/cloud-builders/docker'
    args: ['push', 'gcr.io/${PROJECT_ID}/ott-event-pipeline:latest']
  
  # Create the template spec file
  - name: 'gcr.io/cloud-builders/gcloud'
    entrypoint: '/bin/bash'
    args:
      - '-c'
      - |
        cat > /workspace/event_pipeline_spec.json << EOF
        {
          "image": "gcr.io/${PROJECT_ID}/ott-event-pipeline:latest",
          "sdk_info": {
            "language": "PYTHON"
          },
          "metadata": {
            "name": "OTT Event Processing Pipeline",
            "description": "Process OTT streaming events from Pub/Sub to BigQuery",
            "parameters": [
              {
                "name": "input_subscription",
                "label": "Input PubSub Subscription",
                "helpText": "PubSub subscription to read from"
              },
              {
                "name": "output_table",
                "label": "Output BigQuery Table",
                "helpText": "BigQuery table to write to (PROJECT:DATASET.TABLE)"
              },
              {
                "name": "temp_location",
                "label": "Temp Location",
                "helpText": "GCS location for temporary files"
              }
            ]
          }
        }
        EOF
  
  # Create the Dataflow Flex Template
  - name: 'gcr.io/cloud-builders/gcloud'
    args: [
      'dataflow', 'flex-template', 'build',
      'gs://${_PROCESSED_DATA_BUCKET}/dataflow/templates/event_pipeline.json',
      '--image', 'gcr.io/${PROJECT_ID}/ott-event-pipeline:latest',
      '--sdk-language', 'PYTHON',
      '--metadata-file', '/workspace/event_pipeline_spec.json'
    ]

substitutions:
  _PROCESSED_DATA_BUCKET: "default-bucket-name"

images:
  - 'gcr.io/${PROJECT_ID}/ott-event-pipeline:latest'